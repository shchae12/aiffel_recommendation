{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16117d2",
   "metadata": {},
   "source": [
    "# AutoInt+ 모델 [프로젝트]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ad26a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 호출\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, MaxPooling2D, Conv2D, Dropout, Lambda, Dense, Flatten, Activation, Input, Embedding, BatchNormalization\n",
    "from tensorflow.keras.initializers import glorot_normal, Zeros, TruncatedNormal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import joblib \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e489ef74",
   "metadata": {},
   "source": [
    "## Layer 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb0014",
   "metadata": {},
   "source": [
    "### 임베딩 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d250ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesEmbedding(Layer):  \n",
    "    '''\n",
    "    임베딩 레이어입니다. \n",
    "    - 만약 피처(feature) 3개가 각각 10개, 20개, 30개의 고유값을 가진다면 feature_dims는 [10, 20, 30] 형태를 띄게 됩니다.\n",
    "    - 전체 임베딩을 해야 할 개수는 10+20+30 = 60이므로 '60 x 임베딩_차원_크기'의 행렬이 생성되게 됩니다.\n",
    "    '''\n",
    "    def __init__(self, field_dims, embed_dim, **kwargs):\n",
    "        super(FeaturesEmbedding, self).__init__(**kwargs)\n",
    "        self.total_dim = sum(field_dims)\n",
    "        self.embed_dim = embed_dim\n",
    "        ## int 64로 바꿔주자\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int32)\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=self.total_dim, output_dim=self.embed_dim)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # 임베딩을 빌드하고 초기화합니다.\n",
    "        self.embedding.build(input_shape)\n",
    "        self.embedding.set_weights([tf.keras.initializers.GlorotUniform()(shape=self.embedding.weights[0].shape)])\n",
    "\n",
    "    def call(self, x):\n",
    "        # 들어온 입력의 임베딩을 가져니다.\n",
    "        x = x + tf.constant(self.offsets)\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1bd471",
   "metadata": {},
   "source": [
    "### 다층 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "448982e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiLayerPerceptron(Layer):  \n",
    "    '''\n",
    "    DNN 레이어입니다.\n",
    "    - Tensorflow Keras에서는 Dense 레이어를 쌓아올린 구조입니다.\n",
    "    - 필요에 따라 배치 정규화도 사용할 수 있습니다.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False, init_std=0.0001, output_layer=True):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_bn = use_bn\n",
    "        hidden_units = [input_dim] + list(hidden_units)\n",
    "        if output_layer:\n",
    "            hidden_units += [1]\n",
    "        # Dense layer를 쌓아올립니다.\n",
    "        self.linears = [Dense(units, activation=None, kernel_initializer=tf.random_normal_initializer(stddev=init_std),\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg)) for units in hidden_units[1:]]\n",
    "        # 활성화 함수를 세팅합니다.\n",
    "        self.activation = tf.keras.layers.Activation(activation)\n",
    "        # 필요하다면 배치정규화도 진행합니다.\n",
    "        if self.use_bn:\n",
    "            self.bn = [BatchNormalization() for _ in hidden_units[1:]]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        for i in range(len(self.linears)):\n",
    "            # input data가 들어오면 layer를 돌면서 벡터 값을 가져오게 됩니다.\n",
    "            x = self.linears[i](x)\n",
    "            if self.use_bn:\n",
    "                x = self.bn[i](x, training=training)\n",
    "            # 각 layer마다 나온 벡터 값에 활성화 함수와 dropout을 적용시켜 비선형성 구조와 과적합을 방지합니다.\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99779bfc",
   "metadata": {},
   "source": [
    "### 멀티 헤드 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b0725707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer):  \n",
    "    '''\n",
    "    멀티 헤드 셀프 어텐션 레이어입니다.\n",
    "    - 위에 작성한 수식과 같이 동작됩니다.\n",
    "    - 필요에 따라 잔차 연결(residual connection)도 진행합니다.\n",
    "    '''\n",
    "    def __init__(self, att_embedding_size=8, head_num=2, use_res=True, scaling=False, seed=1024, **kwargs):\n",
    "        if head_num <= 0:\n",
    "            raise ValueError('head_num must be a int > 0')\n",
    "        self.att_embedding_size = att_embedding_size\n",
    "        self.head_num = head_num\n",
    "        self.use_res = use_res\n",
    "        self.seed = seed\n",
    "        self.scaling = scaling\n",
    "        super(MultiHeadSelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if len(input_shape) != 3:\n",
    "            raise ValueError(\n",
    "                \"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (len(input_shape)))\n",
    "        embedding_size = int(input_shape[-1])\n",
    "        # 쿼리에 해당하는 매트릭스입니다. \n",
    "        self.W_Query = self.add_weight(name='query', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                       dtype=tf.float32,\n",
    "                                       initializer=TruncatedNormal(seed=self.seed))\n",
    "        # 키에 해당되는 매트릭스입니다.\n",
    "        self.W_key = self.add_weight(name='key', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=TruncatedNormal(seed=self.seed + 1))\n",
    "        # 값(value)에 해당되는 매트릭스입니다.\n",
    "        self.W_Value = self.add_weight(name='value', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                       dtype=tf.float32,\n",
    "                                       initializer=TruncatedNormal(seed=self.seed + 2))\n",
    "        # 필요하다면 잔차 연결도 할 수 있습니다.\n",
    "        if self.use_res:\n",
    "            self.W_Res = self.add_weight(name='res', shape=[embedding_size, self.att_embedding_size * self.head_num],\n",
    "                                         dtype=tf.float32,\n",
    "                                         initializer=TruncatedNormal(seed=self.seed))\n",
    "\n",
    "        super(MultiHeadSelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if K.ndim(inputs) != 3:\n",
    "            raise ValueError(\"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (K.ndim(inputs)))\n",
    "        \n",
    "        # 입력이 들어오면 쿼리, 키, 값(value)에 매칭되어 각각의 값을 가지고 옵니다.\n",
    "        querys = tf.tensordot(inputs, self.W_Query, axes=(-1, 0))  \n",
    "        keys = tf.tensordot(inputs, self.W_key, axes=(-1, 0))\n",
    "        values = tf.tensordot(inputs, self.W_Value, axes=(-1, 0))\n",
    "\n",
    "        # 헤드 개수에 따라 데이터를 분리해줍니다.\n",
    "        querys = tf.stack(tf.split(querys, self.head_num, axis=2))\n",
    "        keys = tf.stack(tf.split(keys, self.head_num, axis=2))\n",
    "        values = tf.stack(tf.split(values, self.head_num, axis=2))\n",
    "        \n",
    "        # 쿼리와 키를 먼저 곱해줍니다. 위 이미지의 식 (5)와 같습니다.\n",
    "        inner_product = tf.matmul(querys, keys, transpose_b=True)\n",
    "        if self.scaling:\n",
    "            inner_product /= self.att_embedding_size ** 0.5\n",
    "        self.normalized_att_scores =  tf.nn.softmax(inner_product)\n",
    "        \n",
    "        # 쿼리와 키에서 나온 어텐션 값을 값(value)에 곱해줍니다. 식 (6)과 같습니다.\n",
    "        result = tf.matmul(self.normalized_att_scores, values)\n",
    "        # 식 (7)과 같이 쪼개어진 멀테 헤드를 모아줍니다.\n",
    "        result = tf.concat(tf.split(result, self.head_num, ), axis=-1)\n",
    "        result = tf.squeeze(result, axis=0) \n",
    "\n",
    "        if self.use_res:\n",
    "            result += tf.tensordot(inputs, self.W_Res, axes=(-1, 0))\n",
    "        result = tf.nn.relu(result)\n",
    "        \n",
    "        # 그 결과 값을 리턴합니다.\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        return (None, input_shape[1], self.att_embedding_size * self.head_num)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'att_embedding_size': self.att_embedding_size, 'head_num': self.head_num, 'use_res': self.use_res,'seed': self.seed}\n",
    "        base_config = super(MultiHeadSelfAttention, self).get_config()\n",
    "        base_config.update(config)\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7975ed",
   "metadata": {},
   "source": [
    "## AutoInt+ 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4456114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoIntMLP(Layer): \n",
    "    def __init__(self, field_dims, embedding_size, att_layer_num=3, att_head_num=2, att_res=True, dnn_hidden_units=(32, 32), dnn_activation='relu',\n",
    "                 l2_reg_dnn=0, l2_reg_embedding=1e-5, dnn_use_bn=False, dnn_dropout=0.4, init_std=0.0001):\n",
    "        super(AutoIntMLP, self).__init__()\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embedding_size)\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.final_layer = Dense(1, use_bias=False, kernel_initializer=tf.random_normal_initializer(stddev=init_std))\n",
    "        \n",
    "        self.dnn = tf.keras.Sequential()\n",
    "        for units in dnn_hidden_units:\n",
    "            self.dnn.add(Dense(units, activation=dnn_activation,\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l2_reg_dnn),\n",
    "                               kernel_initializer=tf.random_normal_initializer(stddev=init_std)))\n",
    "            if dnn_use_bn:\n",
    "                self.dnn.add(BatchNormalization())\n",
    "            self.dnn.add(Activation(dnn_activation))\n",
    "            if dnn_dropout > 0:\n",
    "                self.dnn.add(Dropout(dnn_dropout))\n",
    "        self.dnn.add(Dense(1, kernel_initializer=tf.random_normal_initializer(stddev=init_std)))\n",
    "\n",
    "        self.int_layers = [MultiHeadSelfAttention(att_embedding_size=embedding_size, head_num=att_head_num, use_res=att_res) for _ in range(att_layer_num)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embed_x = self.embedding(inputs)\n",
    "        dnn_embed = tf.reshape(embed_x, shape=(-1, self.embedding_size * self.num_fields))\n",
    "\n",
    "        att_input = embed_x\n",
    "        for layer in self.int_layers:\n",
    "            att_input = layer(att_input)\n",
    "\n",
    "        att_output = Flatten()(att_input)\n",
    "        att_output = self.final_layer(att_output)\n",
    "        \n",
    "        dnn_output = self.dnn(dnn_embed)\n",
    "        y_pred = tf.keras.activations.sigmoid(att_output + dnn_output)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e170e6",
   "metadata": {},
   "source": [
    "## 평가 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0627a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DCG(ranklist, y_true):\n",
    "    dcg = 0.0\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item in y_true:\n",
    "            dcg += 1.0 / math.log(i + 2)\n",
    "    return  dcg\n",
    "\n",
    "def get_IDCG(ranklist, y_true):\n",
    "    idcg = 0.0\n",
    "    i = 0\n",
    "    for item in y_true:\n",
    "        if item in ranklist:\n",
    "            idcg += 1.0 / math.log(i + 2)\n",
    "            i += 1\n",
    "    return idcg\n",
    "\n",
    "def get_NDCG(ranklist, y_true):\n",
    "    '''NDCG 평가 지표'''\n",
    "    ranklist = np.array(ranklist).astype(int)\n",
    "    y_true = np.array(y_true).astype(int)\n",
    "    dcg = get_DCG(ranklist, y_true)\n",
    "    idcg = get_IDCG(y_true, y_true)\n",
    "    if idcg == 0:\n",
    "        return 0\n",
    "    return round( (dcg / idcg), 5)\n",
    "\n",
    "def get_hit_rate(ranklist, y_true):\n",
    "    '''hitrate 평가 지표'''\n",
    "    c = 0\n",
    "    for y in y_true:\n",
    "        if y in ranklist:\n",
    "            c += 1\n",
    "    return round( c / len(y_true), 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e00287",
   "metadata": {},
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0d2a50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(model, test_df):\n",
    "    '''모델 테스트'''\n",
    "    user_pred_info = defaultdict(list)\n",
    "    total_rows = len(test_df)\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        features = test_df.iloc[i:i + batch_size, :-1].astype(np.int32).values  # 데이터 타입 변경 \n",
    "        y_pred = model.predict(features, verbose=False)\n",
    "        for feature, p in zip(features, y_pred):\n",
    "            u_i = feature[:2]\n",
    "            user_pred_info[int(u_i[0])].append((int(u_i[1]), float(p)))\n",
    "    return user_pred_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910cb09",
   "metadata": {},
   "source": [
    "## 학습 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a0c58415",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = os.path.abspath(os.getcwd())\n",
    "data_dir_nm = 'data'\n",
    "movielens_dir_nm = 'ml-1m'\n",
    "model_dir_nm = 'model'\n",
    "data_path = f\"{project_path}/{data_dir_nm}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "898fc559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000209, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_decade</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>rating_year</th>\n",
       "      <th>rating_month</th>\n",
       "      <th>rating_decade</th>\n",
       "      <th>genre1</th>\n",
       "      <th>genre2</th>\n",
       "      <th>genre3</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>1970s</td>\n",
       "      <td>1975</td>\n",
       "      <td>2000</td>\n",
       "      <td>12</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>1990s</td>\n",
       "      <td>1996</td>\n",
       "      <td>2000</td>\n",
       "      <td>12</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Children's</td>\n",
       "      <td>Musical</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>1960s</td>\n",
       "      <td>1964</td>\n",
       "      <td>2000</td>\n",
       "      <td>12</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Musical</td>\n",
       "      <td>Romance</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>2000s</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>12</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Drama</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>1990s</td>\n",
       "      <td>1998</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>2000s</td>\n",
       "      <td>Animation</td>\n",
       "      <td>Children's</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id movie_id movie_decade movie_year rating_year rating_month  \\\n",
       "0       1     1193        1970s       1975        2000           12   \n",
       "1       1      661        1990s       1996        2000           12   \n",
       "2       1      914        1960s       1964        2000           12   \n",
       "3       1     3408        2000s       2000        2000           12   \n",
       "4       1     2355        1990s       1998        2001            1   \n",
       "\n",
       "  rating_decade     genre1      genre2   genre3 gender age occupation    zip  \\\n",
       "0         2000s      Drama          no       no      F   1         10  48067   \n",
       "1         2000s  Animation  Children's  Musical      F   1         10  48067   \n",
       "2         2000s    Musical     Romance       no      F   1         10  48067   \n",
       "3         2000s      Drama          no       no      F   1         10  48067   \n",
       "4         2000s  Animation  Children's   Comedy      F   1         10  48067   \n",
       "\n",
       "  label  \n",
       "0     1  \n",
       "1     0  \n",
       "2     0  \n",
       "3     1  \n",
       "4     1  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 데이터 불러오기\n",
    "# csv 데이터이므로 read_csv로 가져옵니다.\n",
    "movielens_rcmm = pd.read_csv(f\"{data_path}/movielens_rcmm_v2.csv\", dtype=str)\n",
    "print(movielens_rcmm.shape)\n",
    "movielens_rcmm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e8320811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 라벨 인코더(label encoder)\n",
    "# label은 제외한 각 컬럼을 돌면서 각각의 고윳값들을 0부터 n까지 매핑시킵니다.\n",
    "label_encoders = {col: LabelEncoder() for col in movielens_rcmm.columns[:-1]} # label은 제외\n",
    "\n",
    "for col, le in label_encoders.items():\n",
    "    movielens_rcmm[col] = le.fit_transform(movielens_rcmm[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cb16d0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_decade</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>rating_year</th>\n",
       "      <th>rating_month</th>\n",
       "      <th>rating_decade</th>\n",
       "      <th>genre1</th>\n",
       "      <th>genre2</th>\n",
       "      <th>genre3</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>6</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3374</td>\n",
       "      <td>8</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3615</td>\n",
       "      <td>5</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2503</td>\n",
       "      <td>9</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1374</td>\n",
       "      <td>8</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  movie_decade  movie_year  rating_year  rating_month  \\\n",
       "0        0       189             6          55            0             3   \n",
       "1        0      3374             8          76            0             3   \n",
       "2        0      3615             5          44            0             3   \n",
       "3        0      2503             9          80            0             3   \n",
       "4        0      1374             8          78            1             0   \n",
       "\n",
       "   rating_decade  genre1  genre2  genre3  gender  age  occupation   zip label  \n",
       "0              0       7      17      15       0    0           2  1588     1  \n",
       "1              0       2       2       8       0    0           2  1588     0  \n",
       "2              0      11      12      15       0    0           2  1588     0  \n",
       "3              0       7      17      15       0    0           2  1588     1  \n",
       "4              0       2       2       2       0    0           2  1588     1  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movielens_rcmm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f00d70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_rcmm['label'] = movielens_rcmm['label'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6cb7de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. 학습 데이터와 테스트데이터로 분리, 0.2 정도로 분리\n",
    "train_df, test_df = train_test_split(movielens_rcmm, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ed4b40a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 800167 entries, 416292 to 121958\n",
      "Data columns (total 15 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   user_id        800167 non-null  int64  \n",
      " 1   movie_id       800167 non-null  int64  \n",
      " 2   movie_decade   800167 non-null  int64  \n",
      " 3   movie_year     800167 non-null  int64  \n",
      " 4   rating_year    800167 non-null  int64  \n",
      " 5   rating_month   800167 non-null  int64  \n",
      " 6   rating_decade  800167 non-null  int64  \n",
      " 7   genre1         800167 non-null  int64  \n",
      " 8   genre2         800167 non-null  int64  \n",
      " 9   genre3         800167 non-null  int64  \n",
      " 10  gender         800167 non-null  int64  \n",
      " 11  age            800167 non-null  int64  \n",
      " 12  occupation     800167 non-null  int64  \n",
      " 13  zip            800167 non-null  int64  \n",
      " 14  label          800167 non-null  float32\n",
      "dtypes: float32(1), int64(14)\n",
      "memory usage: 94.6 MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6af39feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6040, 3706,   10,   81,    4,   12,    1,   18,   18,   16,    2,\n",
       "          7,   21, 3439])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요 컬럼들과 레이블 정의\n",
    "# 필드의 각 고유 개수를 정의하는 field_dims를 정의합니다. 이는  임베딩 때 활용됩니다. \n",
    "u_i_feature = ['user_id', 'movie_id']\n",
    "meta_features = ['movie_decade', 'movie_year', 'rating_year', 'rating_month', 'rating_decade', 'genre1','genre2', 'genre3', 'gender', 'age', 'occupation', 'zip']\n",
    "label = 'label'\n",
    "field_dims = np.max(movielens_rcmm[u_i_feature + meta_features].astype(np.int64).values, axis=0) + 1\n",
    "field_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f667f4",
   "metadata": {},
   "source": [
    "## 훈련 환경 및 모델 세팅 -> 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "585d167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 에포크, 학습률, 드롭아웃, 배치사이즈, 임베딩 크기 등 정의\n",
    "# epochs=5\n",
    "# learning_rate= 0.0001\n",
    "# dropout= 0.4\n",
    "# batch_size = 2048\n",
    "# embed_dim= 16\n",
    "\n",
    "# 실험하고 싶은 하이퍼파라미트 리스트 정의 - 어텐션 헤드 수 파라미터 추가\n",
    "experiment_configs = [\n",
    "    {'epochs': 5, 'learning_rate': 0.0001, 'dropout': 0.4, 'batch_size': 2048, 'embed_dim': 16, 'att_head_num': 2}, # 기본\n",
    "    {'epochs': 5, 'learning_rate': 0.0001, 'dropout': 0.4, 'batch_size': 2048, 'embed_dim': 16, 'att_head_num': 4}, # 어텐션 헤드 수 4\n",
    "    {'epochs': 5, 'learning_rate': 0.0001, 'dropout': 0.4, 'batch_size': 2048, 'embed_dim': 16, 'att_head_num': 8}, # 어텐션 헤드 수 8\n",
    "    {'epochs': 3, 'learning_rate': 0.001, 'dropout': 0.5, 'batch_size': 1024, 'embed_dim': 32, 'att_head_num': 4},\n",
    "    {'epochs': 5, 'learning_rate': 0.001, 'dropout': 0.5, 'batch_size': 1024, 'embed_dim': 16, 'att_head_num': 8},\n",
    "    {'epochs': 5, 'learning_rate': 0.001, 'dropout': 0.5, 'batch_size': 1024, 'embed_dim': 32, 'att_head_num': 8}, # 임베딩 사이즈 증가\n",
    "    {'epochs': 5, 'learning_rate': 0.0005, 'dropout': 0.3, 'batch_size': 2048, 'embed_dim': 8, 'att_head_num': 4},\n",
    "    {'epochs': 5, 'learning_rate': 0.0005, 'dropout': 0.3, 'batch_size': 2048, 'embed_dim': 16, 'att_head_num': 8},\n",
    "    {'epochs': 3, 'learning_rate': 0.0001, 'dropout': 0.7, 'batch_size': 1024, 'embed_dim': 16, 'att_head_num': 2},\n",
    "    {'epochs': 3, 'learning_rate': 0.0001, 'dropout': 0.7, 'batch_size': 1024, 'embed_dim': 16, 'att_head_num': 4}    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "010f6f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoInt+ 레이어를 가지고 있는 모델 본체\n",
    "class AutoIntMLPModel(Model):\n",
    "    def __init__(self, field_dims, embedding_size, att_layer_num=3, att_head_num=2,\n",
    "                 att_res=True, dnn_hidden_units=(32, 32), dnn_activation='relu',\n",
    "                 l2_reg_dnn=0, l2_reg_embedding=1e-5, dnn_use_bn=False,\n",
    "                 dnn_dropout=0.4, init_std=0.0001):\n",
    "        super(AutoIntMLPModel, self).__init__()\n",
    "        self.autoInt_layer = AutoIntMLP(\n",
    "            field_dims=field_dims,\n",
    "            embedding_size=embedding_size,\n",
    "            att_layer_num=att_layer_num,\n",
    "            att_head_num=att_head_num,\n",
    "            att_res=att_res,\n",
    "            dnn_hidden_units=dnn_hidden_units,\n",
    "            dnn_activation=dnn_activation,\n",
    "            l2_reg_dnn=l2_reg_dnn,\n",
    "            l2_reg_embedding=l2_reg_embedding,\n",
    "            dnn_use_bn=dnn_use_bn,\n",
    "            dnn_dropout=dnn_dropout,\n",
    "            init_std=init_std\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        return self.autoInt_layer(inputs, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0722d9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Experiment 1: {'epochs': 5, 'learning_rate': 0.0001, 'dropout': 0.4, 'batch_size': 2048, 'embed_dim': 16, 'att_head_num': 2}\n",
      "Epoch 1/5\n",
      "352/352 [==============================] - 9s 23ms/step - loss: 0.6731 - binary_crossentropy: 0.6731 - val_loss: 0.6292 - val_binary_crossentropy: 0.6292\n",
      "Epoch 2/5\n",
      "352/352 [==============================] - 8s 22ms/step - loss: 0.6025 - binary_crossentropy: 0.6025 - val_loss: 0.5833 - val_binary_crossentropy: 0.5833\n",
      "Epoch 3/5\n",
      "352/352 [==============================] - 8s 24ms/step - loss: 0.5611 - binary_crossentropy: 0.5611 - val_loss: 0.5484 - val_binary_crossentropy: 0.5484\n",
      "Epoch 4/5\n",
      "352/352 [==============================] - 8s 23ms/step - loss: 0.5397 - binary_crossentropy: 0.5397 - val_loss: 0.5418 - val_binary_crossentropy: 0.5418\n",
      "Epoch 5/5\n",
      "352/352 [==============================] - 8s 24ms/step - loss: 0.5348 - binary_crossentropy: 0.5348 - val_loss: 0.5396 - val_binary_crossentropy: 0.5396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Experiment 2: {'epochs': 5, 'learning_rate': 0.0001, 'dropout': 0.4, 'batch_size': 2048, 'embed_dim': 16, 'att_head_num': 4}\n",
      "Epoch 1/5\n",
      "352/352 [==============================] - 17s 46ms/step - loss: 0.6589 - binary_crossentropy: 0.6589 - val_loss: 0.6047 - val_binary_crossentropy: 0.6047\n",
      "Epoch 2/5\n",
      "352/352 [==============================] - 16s 46ms/step - loss: 0.5817 - binary_crossentropy: 0.5817 - val_loss: 0.5552 - val_binary_crossentropy: 0.5552\n",
      "Epoch 3/5\n",
      "352/352 [==============================] - 16s 46ms/step - loss: 0.5424 - binary_crossentropy: 0.5424 - val_loss: 0.5419 - val_binary_crossentropy: 0.5419\n",
      "Epoch 4/5\n",
      "352/352 [==============================] - 17s 48ms/step - loss: 0.5351 - binary_crossentropy: 0.5351 - val_loss: 0.5397 - val_binary_crossentropy: 0.5397\n",
      "Epoch 5/5\n",
      "352/352 [==============================] - 17s 48ms/step - loss: 0.5327 - binary_crossentropy: 0.5327 - val_loss: 0.5387 - val_binary_crossentropy: 0.5387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Experiment 3: {'epochs': 5, 'learning_rate': 0.0001, 'dropout': 0.4, 'batch_size': 2048, 'embed_dim': 16, 'att_head_num': 8}\n",
      "Epoch 1/5\n",
      "352/352 [==============================] - 38s 106ms/step - loss: 0.6464 - binary_crossentropy: 0.6464 - val_loss: 0.5830 - val_binary_crossentropy: 0.5830\n",
      "Epoch 2/5\n",
      "352/352 [==============================] - 36s 102ms/step - loss: 0.5502 - binary_crossentropy: 0.5502 - val_loss: 0.5422 - val_binary_crossentropy: 0.5422\n",
      "Epoch 3/5\n",
      "352/352 [==============================] - 35s 100ms/step - loss: 0.5359 - binary_crossentropy: 0.5359 - val_loss: 0.5396 - val_binary_crossentropy: 0.5396\n",
      "Epoch 4/5\n",
      "352/352 [==============================] - 35s 100ms/step - loss: 0.5331 - binary_crossentropy: 0.5331 - val_loss: 0.5380 - val_binary_crossentropy: 0.5380\n",
      "Epoch 5/5\n",
      "352/352 [==============================] - 35s 100ms/step - loss: 0.5319 - binary_crossentropy: 0.5319 - val_loss: 0.5389 - val_binary_crossentropy: 0.5389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Experiment 4: {'epochs': 3, 'learning_rate': 0.001, 'dropout': 0.5, 'batch_size': 1024, 'embed_dim': 32, 'att_head_num': 4}\n",
      "Epoch 1/3\n",
      "704/704 [==============================] - 37s 52ms/step - loss: 0.5617 - binary_crossentropy: 0.5617 - val_loss: 0.5362 - val_binary_crossentropy: 0.5362\n",
      "Epoch 2/3\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.5306 - binary_crossentropy: 0.5306 - val_loss: 0.5314 - val_binary_crossentropy: 0.5314\n",
      "Epoch 3/3\n",
      "704/704 [==============================] - 36s 52ms/step - loss: 0.5212 - binary_crossentropy: 0.5212 - val_loss: 0.5249 - val_binary_crossentropy: 0.5249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Experiment 5: {'epochs': 5, 'learning_rate': 0.001, 'dropout': 0.5, 'batch_size': 1024, 'embed_dim': 16, 'att_head_num': 8}\n",
      "Epoch 1/5\n",
      "704/704 [==============================] - 40s 55ms/step - loss: 0.5632 - binary_crossentropy: 0.5632 - val_loss: 0.5381 - val_binary_crossentropy: 0.5381\n",
      "Epoch 2/5\n",
      "704/704 [==============================] - 40s 57ms/step - loss: 0.5329 - binary_crossentropy: 0.5329 - val_loss: 0.5316 - val_binary_crossentropy: 0.5316\n",
      "Epoch 3/5\n",
      "704/704 [==============================] - 39s 56ms/step - loss: 0.5243 - binary_crossentropy: 0.5243 - val_loss: 0.5266 - val_binary_crossentropy: 0.5266\n",
      "Epoch 4/5\n",
      "704/704 [==============================] - 40s 57ms/step - loss: 0.5168 - binary_crossentropy: 0.5168 - val_loss: 0.5225 - val_binary_crossentropy: 0.5225\n",
      "Epoch 5/5\n",
      "704/704 [==============================] - 40s 57ms/step - loss: 0.5088 - binary_crossentropy: 0.5088 - val_loss: 0.5201 - val_binary_crossentropy: 0.5201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Experiment 6: {'epochs': 5, 'learning_rate': 0.001, 'dropout': 0.5, 'batch_size': 1024, 'embed_dim': 32, 'att_head_num': 8}\n",
      "Epoch 1/5\n",
      "704/704 [==============================] - 91s 128ms/step - loss: 0.5599 - binary_crossentropy: 0.5599 - val_loss: 0.5353 - val_binary_crossentropy: 0.5353\n",
      "Epoch 2/5\n",
      "704/704 [==============================] - 91s 129ms/step - loss: 0.5299 - binary_crossentropy: 0.5299 - val_loss: 0.5285 - val_binary_crossentropy: 0.5285\n",
      "Epoch 3/5\n",
      "704/704 [==============================] - 91s 129ms/step - loss: 0.5190 - binary_crossentropy: 0.5190 - val_loss: 0.5219 - val_binary_crossentropy: 0.5219\n",
      "Epoch 4/5\n",
      "704/704 [==============================] - 90s 128ms/step - loss: 0.5077 - binary_crossentropy: 0.5077 - val_loss: 0.5204 - val_binary_crossentropy: 0.5204\n",
      "Epoch 5/5\n",
      "704/704 [==============================] - 89s 127ms/step - loss: 0.4976 - binary_crossentropy: 0.4976 - val_loss: 0.5174 - val_binary_crossentropy: 0.5174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Experiment 7: {'epochs': 5, 'learning_rate': 0.0005, 'dropout': 0.3, 'batch_size': 2048, 'embed_dim': 8, 'att_head_num': 4}\n",
      "Epoch 1/5\n",
      "352/352 [==============================] - 9s 25ms/step - loss: 0.6211 - binary_crossentropy: 0.6211 - val_loss: 0.5509 - val_binary_crossentropy: 0.5509\n",
      "Epoch 2/5\n",
      "352/352 [==============================] - 8s 23ms/step - loss: 0.5412 - binary_crossentropy: 0.5412 - val_loss: 0.5393 - val_binary_crossentropy: 0.5393\n",
      "Epoch 3/5\n",
      "352/352 [==============================] - 8s 24ms/step - loss: 0.5339 - binary_crossentropy: 0.5339 - val_loss: 0.5370 - val_binary_crossentropy: 0.5370\n",
      "Epoch 4/5\n",
      "352/352 [==============================] - 8s 23ms/step - loss: 0.5305 - binary_crossentropy: 0.5305 - val_loss: 0.5354 - val_binary_crossentropy: 0.5354\n",
      "Epoch 5/5\n",
      "352/352 [==============================] - 8s 24ms/step - loss: 0.5268 - binary_crossentropy: 0.5268 - val_loss: 0.5320 - val_binary_crossentropy: 0.5320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Experiment 8: {'epochs': 5, 'learning_rate': 0.0005, 'dropout': 0.3, 'batch_size': 2048, 'embed_dim': 16, 'att_head_num': 8}\n",
      "Epoch 1/5\n",
      "352/352 [==============================] - 36s 99ms/step - loss: 0.5845 - binary_crossentropy: 0.5845 - val_loss: 0.5420 - val_binary_crossentropy: 0.5420\n",
      "Epoch 2/5\n",
      "352/352 [==============================] - 35s 99ms/step - loss: 0.5377 - binary_crossentropy: 0.5377 - val_loss: 0.5392 - val_binary_crossentropy: 0.5392\n",
      "Epoch 3/5\n",
      "352/352 [==============================] - 35s 98ms/step - loss: 0.5315 - binary_crossentropy: 0.5315 - val_loss: 0.5320 - val_binary_crossentropy: 0.5320\n",
      "Epoch 4/5\n",
      "352/352 [==============================] - 35s 98ms/step - loss: 0.5248 - binary_crossentropy: 0.5248 - val_loss: 0.5294 - val_binary_crossentropy: 0.5294\n",
      "Epoch 5/5\n",
      "352/352 [==============================] - 36s 101ms/step - loss: 0.5202 - binary_crossentropy: 0.5202 - val_loss: 0.5273 - val_binary_crossentropy: 0.5273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Experiment 9: {'epochs': 3, 'learning_rate': 0.0001, 'dropout': 0.7, 'batch_size': 1024, 'embed_dim': 16, 'att_head_num': 2}\n",
      "Epoch 1/3\n",
      "704/704 [==============================] - 11s 14ms/step - loss: 0.6457 - binary_crossentropy: 0.6457 - val_loss: 0.5892 - val_binary_crossentropy: 0.5892\n",
      "Epoch 2/3\n",
      "704/704 [==============================] - 10s 14ms/step - loss: 0.5575 - binary_crossentropy: 0.5575 - val_loss: 0.5442 - val_binary_crossentropy: 0.5442\n",
      "Epoch 3/3\n",
      "704/704 [==============================] - 13s 19ms/step - loss: 0.5374 - binary_crossentropy: 0.5374 - val_loss: 0.5399 - val_binary_crossentropy: 0.5399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Experiment 10: {'epochs': 3, 'learning_rate': 0.0001, 'dropout': 0.7, 'batch_size': 1024, 'embed_dim': 16, 'att_head_num': 4}\n",
      "Epoch 1/3\n",
      "704/704 [==============================] - 18s 24ms/step - loss: 0.6289 - binary_crossentropy: 0.6289 - val_loss: 0.5591 - val_binary_crossentropy: 0.5591\n",
      "Epoch 2/3\n",
      "704/704 [==============================] - 16s 22ms/step - loss: 0.5440 - binary_crossentropy: 0.5440 - val_loss: 0.5412 - val_binary_crossentropy: 0.5412\n",
      "Epoch 3/3\n",
      "704/704 [==============================] - 16s 22ms/step - loss: 0.5356 - binary_crossentropy: 0.5356 - val_loss: 0.5390 - val_binary_crossentropy: 0.5390\n",
      "\n",
      " 모든 실험 학습 완료.\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의 -> 학습 반복\n",
    "trained_models = []\n",
    "model_configs = []\n",
    "\n",
    "for i, config in enumerate(experiment_configs):\n",
    "    print(f\"\\n▶ Experiment {i+1}: {config}\")\n",
    "\n",
    "    autoIntMLP_model = AutoIntMLPModel(\n",
    "        field_dims=field_dims,\n",
    "        embedding_size=config['embed_dim'],\n",
    "        att_layer_num=3,\n",
    "        att_head_num=config['att_head_num'],\n",
    "        att_res=True,\n",
    "        dnn_hidden_units=(32, 32),               # 추가: DNN 은닉층 구조\n",
    "        dnn_activation='relu',                  # 추가: 활성화 함수\n",
    "        l2_reg_dnn=0,\n",
    "        l2_reg_embedding=1e-5,\n",
    "        dnn_use_bn=False,\n",
    "        dnn_dropout=config['dropout'],\n",
    "        init_std=0.0001\n",
    "        )\n",
    "\n",
    "    autoIntMLP_model.compile(\n",
    "        optimizer=Adam(learning_rate=config['learning_rate']),\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=['binary_crossentropy']\n",
    "    )\n",
    "\n",
    "    X = train_df[u_i_feature + meta_features].astype(np.int32)\n",
    "    y = train_df[label]\n",
    "\n",
    "    autoIntMLP_model.fit(X, y,\n",
    "              epochs=config['epochs'],\n",
    "              batch_size=config['batch_size'],\n",
    "              validation_split=0.1\n",
    "              )\n",
    "\n",
    "    # 저장용 리스트에 모델과 설정 기록\n",
    "    trained_models.append(autoIntMLP_model)\n",
    "    model_configs.append(config)\n",
    "\n",
    "print(\"\\n 모든 실험 학습 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3317a08d",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2636c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating model 1: {'epochs': 5, 'learning_rate': 0.0001, 'dropout': 0.4, 'batch_size': 2048, 'embed_dim': 16, 'att_head_num': 2}\n",
      "결과: NDCG=0.6616, Hitrate=0.63023\n",
      "\n",
      " Evaluating model 2: {'epochs': 5, 'learning_rate': 0.0001, 'dropout': 0.4, 'batch_size': 2048, 'embed_dim': 16, 'att_head_num': 4}\n",
      "결과: NDCG=0.66144, Hitrate=0.63016\n",
      "\n",
      " Evaluating model 3: {'epochs': 5, 'learning_rate': 0.0001, 'dropout': 0.4, 'batch_size': 2048, 'embed_dim': 16, 'att_head_num': 8}\n",
      "결과: NDCG=0.66154, Hitrate=0.6302\n",
      "\n",
      " Evaluating model 4: {'epochs': 3, 'learning_rate': 0.001, 'dropout': 0.5, 'batch_size': 1024, 'embed_dim': 32, 'att_head_num': 4}\n",
      "결과: NDCG=0.66707, Hitrate=0.63379\n",
      "\n",
      " Evaluating model 5: {'epochs': 5, 'learning_rate': 0.001, 'dropout': 0.5, 'batch_size': 1024, 'embed_dim': 16, 'att_head_num': 8}\n",
      "결과: NDCG=0.67171, Hitrate=0.63594\n",
      "\n",
      " Evaluating model 6: {'epochs': 5, 'learning_rate': 0.001, 'dropout': 0.5, 'batch_size': 1024, 'embed_dim': 32, 'att_head_num': 8}\n",
      "결과: NDCG=0.67482, Hitrate=0.63843\n",
      "\n",
      " Evaluating model 7: {'epochs': 5, 'learning_rate': 0.0005, 'dropout': 0.3, 'batch_size': 2048, 'embed_dim': 8, 'att_head_num': 4}\n",
      "결과: NDCG=0.66186, Hitrate=0.63037\n",
      "\n",
      " Evaluating model 8: {'epochs': 5, 'learning_rate': 0.0005, 'dropout': 0.3, 'batch_size': 2048, 'embed_dim': 16, 'att_head_num': 8}\n",
      "결과: NDCG=0.66527, Hitrate=0.63177\n",
      "\n",
      " Evaluating model 9: {'epochs': 3, 'learning_rate': 0.0001, 'dropout': 0.7, 'batch_size': 1024, 'embed_dim': 16, 'att_head_num': 2}\n",
      "결과: NDCG=0.66213, Hitrate=0.63042\n",
      "\n",
      " Evaluating model 10: {'epochs': 3, 'learning_rate': 0.0001, 'dropout': 0.7, 'batch_size': 1024, 'embed_dim': 16, 'att_head_num': 4}\n",
      "결과: NDCG=0.66147, Hitrate=0.62991\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, (model, config) in enumerate(zip(trained_models, model_configs)):\n",
    "    print(f\"\\n Evaluating model {i+1}: {config}\")\n",
    "    \n",
    "    user_pred_info = {}\n",
    "    mymodel_user_pred_info = test_model(model, test_df)\n",
    "\n",
    "    for user, data_info in mymodel_user_pred_info.items():\n",
    "        ranklist = sorted(data_info, key=lambda s: s[1], reverse=True)[:10]\n",
    "        ranklist = list(dict.fromkeys([r[0] for r in ranklist]))\n",
    "        user_pred_info[str(user)] = ranklist\n",
    "\n",
    "    test_data = test_df[test_df['label'] == 1].groupby('user_id')['movie_id'].apply(list)\n",
    "\n",
    "    mymodel_ndcg_result = {}\n",
    "    mymodel_hitrate_result = {}\n",
    "\n",
    "    for user, data_info in test_data.items():\n",
    "        mymodel_pred = user_pred_info.get(str(user), [])[:10]\n",
    "        testset = list(set(np.array(data_info).astype(int)))\n",
    "\n",
    "        mymodel_ndcg_result[user] = get_NDCG(mymodel_pred, testset)\n",
    "        mymodel_hitrate_result[user] = get_hit_rate(mymodel_pred, testset)\n",
    "\n",
    "    ndcg = round(np.mean(list(mymodel_ndcg_result.values())), 5)\n",
    "    hitrate = round(np.mean(list(mymodel_hitrate_result.values())), 5)\n",
    "\n",
    "    print(f\"결과: NDCG={ndcg}, Hitrate={hitrate}\")\n",
    "\n",
    "    results.append({\n",
    "        \"Config\": f\"Exp_{i+1}\",\n",
    "        \"Epochs\": config[\"epochs\"],\n",
    "        \"LR\": config[\"learning_rate\"],\n",
    "        \"Dropout\": config[\"dropout\"],\n",
    "        \"Batch\": config[\"batch_size\"],\n",
    "        \"Embed\": config[\"embed_dim\"],\n",
    "        \"AttHead\":config[\"att_head_num\"],\n",
    "        \"NDCG\": ndcg,\n",
    "        \"Hitrate\": hitrate\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"NDCG\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "204bca2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실험 결과 요약표\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Config</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>LR</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Batch</th>\n",
       "      <th>Embed</th>\n",
       "      <th>AttHead</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>Hitrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exp_6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1024</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.67482</td>\n",
       "      <td>0.63843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exp_5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1024</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.67171</td>\n",
       "      <td>0.63594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exp_4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1024</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.66707</td>\n",
       "      <td>0.63379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exp_8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2048</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.66527</td>\n",
       "      <td>0.63177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Exp_9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1024</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.66213</td>\n",
       "      <td>0.63042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Exp_7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2048</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.66186</td>\n",
       "      <td>0.63037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Exp_1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2048</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.66160</td>\n",
       "      <td>0.63023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Exp_3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2048</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.66154</td>\n",
       "      <td>0.63020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Exp_10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1024</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.66147</td>\n",
       "      <td>0.62991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Exp_2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2048</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.66144</td>\n",
       "      <td>0.63016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Config  Epochs      LR  Dropout  Batch  Embed  AttHead     NDCG  Hitrate\n",
       "0   Exp_6       5  0.0010      0.5   1024     32        8  0.67482  0.63843\n",
       "1   Exp_5       5  0.0010      0.5   1024     16        8  0.67171  0.63594\n",
       "2   Exp_4       3  0.0010      0.5   1024     32        4  0.66707  0.63379\n",
       "3   Exp_8       5  0.0005      0.3   2048     16        8  0.66527  0.63177\n",
       "4   Exp_9       3  0.0001      0.7   1024     16        2  0.66213  0.63042\n",
       "5   Exp_7       5  0.0005      0.3   2048      8        4  0.66186  0.63037\n",
       "6   Exp_1       5  0.0001      0.4   2048     16        2  0.66160  0.63023\n",
       "7   Exp_3       5  0.0001      0.4   2048     16        8  0.66154  0.63020\n",
       "8  Exp_10       3  0.0001      0.7   1024     16        4  0.66147  0.62991\n",
       "9   Exp_2       5  0.0001      0.4   2048     16        4  0.66144  0.63016"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"실험 결과 요약표\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f9ffbf",
   "metadata": {},
   "source": [
    "## 베스트 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c3709b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model (Exp_6) 저장 완료: {'epochs': 5, 'learning_rate': 0.001, 'dropout': 0.5, 'batch_size': 1024, 'embed_dim': 32, 'att_head_num': 8}\n"
     ]
    }
   ],
   "source": [
    "# 1등 모델의 Config 값\n",
    "best_exp_name = results_df.loc[0, \"Config\"]\n",
    "\n",
    "# 1등 모델의 index\n",
    "best_idx = int(best_exp_name.split(\"_\")[1]) - 1\n",
    "\n",
    "# 저장할 모델\n",
    "best_model = trained_models[best_idx]\n",
    "best_config = model_configs[best_idx]\n",
    "\n",
    "# 저장 로직\n",
    "np.save('./data/autoIntMLP_field_dims.npy', field_dims)\n",
    "best_model.save_weights('./model/autoIntMLP_model_weights.h5')\n",
    "joblib.dump(label_encoders, './data/autoIntMLP_label_encoders.pkl')\n",
    "\n",
    "print(f\"\\nBest Model ({best_exp_name}) 저장 완료: {best_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee73417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcm_aiffel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
